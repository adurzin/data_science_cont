1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации:
 micro, macro, weighted?

	Ипользуются для усреднения результатов многоклассовых выборок всех классов.
	
	macro вычисляет значения точности (Precision, Recall, F1-score) по отдельности, затем находит среднее значение F1-score (F1 + F2 + F3)/3.
	
	weighted работает как macro, но к каждому классу добавляется коэффициент по его присутствию в истинной выборке данных из 100%.
	
	micro считает общие значения точности (Precision, Recall, F1-score) для всех классов.


2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
	
	XGBoost - алгоритм дерева решений, основанный на методе предварительной сортировки (за счет регуляризации, а также за счет отсечения листьев дерева (Information Gain)). Листья деревьев отсекаются после построения дерева на заданную глубину.
	
	Lightgbm - Потребляет меньшую память и работает быстрее, чем XGBoost. Выбирает из выборки наблюдения с большей ошибкой от таргетных значений и небольшую часть наблюдений с маленькой ошибкой, за счет этого и уменьшается потребление памяти, а скорость увеличивается (в расчете Information gain вводится дополнительный коэффициент, который рассчитывается в зависмости от количества присутствующих наблюдений в бустинге от общего числа наблюдений). Также в LGBM возможна работа с категориальными признаками.
	
	CatBoost делит наблюдения на обучающие и тренирующие. Работает на основе симметричных деревьев, выявляет самые лучшие параметры для разделения. CATBoost лучше остальных работает с категориальными функциями, в то время как XGBoost принимает только числовые данные.